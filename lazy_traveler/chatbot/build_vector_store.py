import os
import math
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document
import openai
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API key ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")


def build_vector_store():
    try:
        current_dir = os.getcwd()
        print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
        
        chatbot_dir = os.path.join(current_dir, "chatbot")  # chatbot ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
        txt_folder = os.path.join(chatbot_dir, "txt_folder")  # chatbot/txt_folder ê²½ë¡œ
        print(f"í…ìŠ¤íŠ¸ íŒŒì¼ í´ë” ê²½ë¡œ: {txt_folder}")

        # í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(txt_folder):
            print(f"âŒ ê²½ë¡œ ì˜¤ë¥˜: {txt_folder} í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… í…ìŠ¤íŠ¸ í´ë” ê²½ë¡œ: {txt_folder}")

        
        # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ í™•ì¸
        vector_store_path = os.path.join(current_dir, "vector_store")
        print(f"ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ: {vector_store_path}")
        
        # 1. í´ë” ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
        all_docs = []
        for filename in os.listdir(txt_folder):
            if filename.endswith(".txt"):
                txt_file_path = os.path.join(txt_folder, filename)
                try:
                    with open(txt_file_path, "r", encoding="utf-8") as file:
                        content = file.read()
                        print(f"íŒŒì¼ ë¡œë”© ì„±ê³µ: {filename}")
                        print(f"íŒŒì¼ ë‚´ìš©ì˜ ì²« 100ì: {content[:100]}")
                        
                        # 2. ë¬¸ì„œ chunking í•˜ê¸°
                        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=350)
                        splits = text_splitter.split_text(content)
                        print(f"ì²­í¬ ìˆ˜: {len(splits)}")

                        # 3. ë¬¸ìì—´ì„ Document ê°ì²´ë¡œ ë³€í™˜
                        docs = [Document(page_content=split) for split in splits]
                        all_docs.extend(docs)
                        
                except Exception as e:
                    print(f"íŒŒì¼ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

        print(f"ì´ {len(all_docs)} ê°œì˜ Document ê°ì²´ ìˆ˜ì§‘ ì™„ë£Œ.")

        # 4. embedding ë„êµ¬ ì„¤ì •
        embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
        
        # 5. Chroma ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
        vector_store = Chroma(persist_directory=vector_store_path, embedding_function=embeddings)

        # 6. ë°°ì¹˜ í¬ê¸° ì¡°ì • ë° ë²¡í„° ì²˜ë¦¬
        batch_size = 166  # ìµœëŒ€ ë°°ì¹˜ í¬ê¸°
        num_batches = math.ceil(len(all_docs) / batch_size)  # ì´ ë°°ì¹˜ ìˆ˜ ê³„ì‚°

        for i in range(num_batches):
            batch_start = i * batch_size
            batch_end = min((i + 1) * batch_size, len(all_docs))
            batch = all_docs[batch_start:batch_end]
            
            # ë°°ì¹˜ì— ëŒ€í•´ ì„ë² ë”© ì²˜ë¦¬
            print(f"ë°°ì¹˜ {i+1}/{num_batches} ì²˜ë¦¬ ì¤‘, ë¬¸ì„œ ìˆ˜: {len(batch)}")
            vectors = embeddings.embed_documents([doc.page_content for doc in batch])

            # ë²¡í„°ì™€ ë¬¸ì„œ ë°ì´í„°ë¥¼ Chromaì— ì¶”ê°€
            # ë°°ì¹˜ì˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€: íŒŒì¼ëª…, ì²­í¬ ë²ˆí˜¸
            vector_store.add_texts([doc.page_content for doc in batch],
                                   metadatas=[{"filename": txt_file_path, "chunk_id": idx} for idx in range(len(batch))])

            # ë””ë²„ê¹…ì„ ìœ„í•œ ë°°ì¹˜ë³„ ìƒíƒœ ì¶œë ¥
            print(f"ë°°ì¹˜ {i+1}/{num_batches} ì™„ë£Œ")

        # ì €ì¥ í™•ì¸
        print(f"\nğŸ“‚ ë²¡í„° ìŠ¤í† ì–´ ë¬¸ì„œ ìˆ˜: {vector_store._collection.count()}")
        print(f"ğŸ“‚ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ: {vector_store._persist_directory}")
        
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")


if __name__ == "__main__":  
    build_vector_store()
