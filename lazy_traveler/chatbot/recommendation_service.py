from datetime import datetime
from langchain.chains import LLMChain
from .prompt import function_prompt, place_prompt, query_prompt
from .utils import (
    get_context,
    get_user_tags,
    sort_places_by_distance,
    schedule_to_text,
    build_schedule_by_categories_with_preferences,
    determine_schedule_template,
    get_preferred_tags_by_schedule,
    search_places_by_preferred_tags,
    classify_question_with_llm,
    format_place_results_to_html,
    filter_open_places_with_llm,
    search_places
    )
from .openai_chroma_config import function_vector_store, llm



async def get_recommendation(user_query, session_id=None, username=None, latitude=None, longitude=None):
    now = datetime.now()
    # now = datetime(2025, 3, 27, 9, 0, 0)
    current_time = now.strftime("%Y-%m-%d %H:%M:%S")
    start_time = now

    if latitude is None or longitude is None:
        latitude, longitude = 37.5704, 126.9831

    #ìœ ì € ì§ˆë¬¸ ê¸°ëŠ¥ ë¶„ë¥˜(llm)
    question_type = await classify_question_with_llm(user_query)

    if question_type == "function":
        function_results = function_vector_store.similarity_search_with_score(
            query=user_query,
            k=1,
            filter={"type": "qa"}
        )
        # print("function_results[0][1]:",function_results[0][1])
        if not function_results or function_results[0][1] > 1.1:
            return "ê¸°ëŠ¥ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        answer = function_results[0][0].metadata.get("answer", "")
        return answer
    
    if question_type == "place":
        latitude = float(latitude)
        longitude = float(longitude)
        place_results = await search_places(user_query, latitude, longitude) #place ê²€ìƒ‰ ë° ê±°ë¦¬ ê³„ì‚°
        # print("place_results:", place_results)

        # ê²°ê³¼ í¬ë§·íŒ… (HTMLë¡œ ë³€í™˜)
        place_results_html = await format_place_results_to_html(place_results) #place ê²°ê³¼ htmlë¡œ ë³€í™˜ 
        return place_results_html

        

    if question_type == "unknown":
        error_message = "ì£„ì†¡í•©ë‹ˆë‹¤.ğŸ˜¢ ê¸°ëŠ¥, ì¥ì†Œ, ì¼ì • ìŠ¤ì¼€ì¤„ë§ì— ëŒ€í•´ ë¬¸ì˜í•´ ì£¼ì„¸ìš”. ğŸ˜Šì˜ˆ) íšŒì›ê°€ì… í•˜ëŠ” ë²•â€˜, â€˜ìŠ¤ì¼€ì¤„ë§ í•´ì¤˜â€˜, â€˜ë§›ì§‘ ì¶”ì²œí•´ì¤˜â€™"
        return error_message

    #ìŠ¤ì¼€ì¤„ë§ ì‹œê°„ëŒ€
    schedule_type, schedule_categories = await determine_schedule_template(now) #ì‹œê°„ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ë§í‘œ ì§€ì •
    # print("schedule_categories:", schedule_categories)
    if schedule_type == "ë¶ˆê°€ì‹œê°„":
        return "ìŠ¤ì¼€ì¤„ë§ ë¶ˆê°€ì‹œê°„ì…ë‹ˆë‹¤. ì˜¤ì „ 8ì‹œë¶€í„° ì˜¤í›„ 11ì‹œê¹Œì§€ë§Œ ìŠ¤ì¼€ì¤„ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    
    # íƒœê·¸ ê°€ì ¸ì˜¤ê¸°
    user_tags = await get_user_tags(username) # ìœ ì € íƒœê·¸ ê°€ì ¸ì˜¤ê¸°
    preferred_tag_mapping = await get_preferred_tags_by_schedule(user_tags, schedule_categories)

    # query_transform_chain = LLMChain(llm=llm, prompt=query_prompt)
    # # 1. ë¹„ë™ê¸° ì‹¤í–‰ í›„ ê²°ê³¼ ì €ì¥
    # transformed_query_result = await query_transform_chain.ainvoke({"query": user_query})

    # # 2. ë³€í™˜ëœ ì¿¼ë¦¬ ì¶”ì¶œ
    # transformed_query = transformed_query_result['text']
    # print(f"[DEBUG] ë³€í™˜ëœ ì¿¼ë¦¬: {transformed_query}")

    # ë¬¸ì„œ ê²€ìƒ‰
    # search_query = f"{user_query} (ìœ„ì¹˜: {latitude}, {longitude}) ê´€ë ¨ íƒœê·¸: {user_tags}"

    docs = await search_places_by_preferred_tags(user_query, preferred_tag_mapping)

    # ê±°ë¦¬ ì •ë ¬
    sorted_docs = await sort_places_by_distance(docs, latitude, longitude)
    # print("sorted_docs:",sorted_docs)

    #ìš´ì˜ì‹œê°„ Let's go
    filtered_docs = await filter_open_places_with_llm(sorted_docs, now)
    # print("filtered_docs:", filtered_docs)

    schedule = await build_schedule_by_categories_with_preferences(
        filtered_docs, schedule_categories, preferred_tag_mapping, start_time
    )
    # print("schedule1:", schedule )


    # 5. ìŠ¤ì¼€ì¤„ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    schedule_text = await schedule_to_text(schedule)
    # print("schedule_text:", schedule_text )

    # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
    context = await get_context(session_id)
    context += f"\n{schedule_text}"

    # âœ… LLM ëª¨ë¸ ì„¤ì •
    chain = place_prompt | llm

    # LLM í˜¸ì¶œ
    result = await chain.ainvoke({
        "context": context,
        "location_context": f"í˜„ì¬ ì‚¬ìš©ìì˜ ìœ„ì¹˜ëŠ” ìœ„ë„ {latitude}, ê²½ë„ {longitude}ì…ë‹ˆë‹¤.",
        "time_context": f"í˜„ì¬ ì‹œê°„ì€ {current_time}ì…ë‹ˆë‹¤.",
        "question": user_query
    })
    # print("result:", result)

    return result.content.strip() if result.content else "ì¶”ì²œì„ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."