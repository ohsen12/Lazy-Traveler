from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
import openai
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API key ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")

# 1. ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ ì§€ì •
base_dir = os.path.dirname(os.path.abspath(__file__))
vector_store_path = os.path.join(base_dir, "vector_store")

# 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

# 3. ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
vectorstore = Chroma(persist_directory=vector_store_path, embedding_function=embeddings)

# 4. Retriever ì„¤ì • (ê²€ìƒ‰ ê¸°ëŠ¥ë§Œ ë‹´ë‹¹)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # ê´€ë ¨ ë¬¸ì„œ 3ê°œ ê²€ìƒ‰

# 5. LLM (GPT ëª¨ë¸) ì„¤ì •
model = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

# 6. parser
output_parser = StrOutputParser()

# 7. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
prompt = PromptTemplate(
    template="ë„ˆëŠ” ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬ ë¬¸ì„œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´.\n\n"
             "ì§ˆë¬¸: {question}\n\n"
             "ì°¸ê³  ë¬¸ì„œ:\n{context}\n\n"
             "ë‹µë³€:",
    input_variables=["question", "context"]
)

# 8. Runnable ì²´ì¸ ì •ì˜
chain = (
    {
        "context": retriever | RunnablePassthrough(),  # ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ ë„˜ê²¨ì¤Œ
        "question": RunnablePassthrough()  # ì‚¬ìš©ìì˜ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ì „ë‹¬
    }
    | prompt  # í”„ë¡¬í”„íŠ¸ ì ìš©
    | model  # GPT ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
    | output_parser  # ë¬¸ìì—´ë¡œ ë³€í™˜
)

# 8. ì§ˆë¬¸ ì •ì˜
query = "ë‚˜ ì§€ê¸ˆ ë¶ì´Œí•œì˜¥ë§ˆì„ì¸ë° ê°ˆë§Œí•œ ë² ì´ì»¤ë¦¬ ì¹´í˜ ì•Œë ¤ì¤˜."

# 9. ì •ì œëœ ë‹µë³€ì„ result ì— ë‹´ìŒ
result = chain.invoke(query)

# 10. ë‹µë³€ ì¶œë ¥
print(f"\nğŸ¤– AI ë‹µë³€:\n{result}\n")